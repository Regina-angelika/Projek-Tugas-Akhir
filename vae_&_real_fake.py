# -*- coding: utf-8 -*-
"""VAE & Real Fake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxRyiCcPBAemB1iOLjUqfN-h-7sKa8ZV

###Tensorflow-GPU
"""

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)

# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

"""###VAE Image Generate

####Dataset CelebA Download
"""

!pip install opendatasets

import opendatasets as od
od.download('https://www.kaggle.com/datasets/jessicali9530/celeba-dataset')

"""####Dataset VAE"""

import numpy as np
import pandas as pd
import os
import shutil
import errno
import matplotlib.pyplot as plt
import seaborn as sns

from skimage.io import imread

def get_input(path):
    """get specific image from path"""
    img = imread(path)
    return img

def get_output(path, label_file = None):
    """get all the labels relative to the image of path"""
    img_id = path.split('/')[-1]
    labels = label_file.loc[img_id].values
    return labels

def preprocess_input(img):
    # convert between 0 and 1
    return img.astype('float32') / 127.5 -1

def image_generator(files, label_file, batch_size = 32):
    while True:

        batch_paths = np.random.choice(a = files, size = batch_size)
        batch_input = []
        batch_output = []

        for input_path in batch_paths:

            input = get_input(input_path)
            input = preprocess_input(input)
            output = get_output(input_path, label_file = label_file)
            batch_input += [input]
            batch_output += [output]
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)

        yield batch_x, batch_y

def auto_encoder_generator(files, batch_size = 32):
    while True:
        batch_paths = np.random.choice(a = files, size = batch_size)
        batch_input = []
        batch_output = []

        for input_path in batch_paths:
            input = get_input(input_path)
            input = preprocess_input(input)
            output = input
            batch_input += [input]
            batch_output += [output]
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)

        yield batch_x, batch_y

attr = pd.read_csv('/content/celeba-dataset/list_attr_celeba.csv')
attr = attr.set_index('image_id')

# check if attribute successful loaded
attr.describe()

from sklearn.model_selection import train_test_split
IMG_NAME_LENGTH = 6
file_path = "/content/celeba-dataset/img_align_celeba/img_align_celeba/"
img_id = np.arange(1,len(attr.index)+1)
img_path = []
for i in range(len(img_id)):
    img_path.append(file_path + (IMG_NAME_LENGTH - len(str(img_id[i])))*'0' + str(img_id[i]) + '.jpg')

# pick 80% as training set and 20% as validation set
train_path = img_path[:int((0.8)*len(img_path))]
val_path = img_path[int((0.8)*len(img_path)):]
train_generator = auto_encoder_generator(train_path,32)
val_generator = auto_encoder_generator(val_path,32)

fig, ax = plt.subplots(1, 5, figsize=(12, 4))
for i in range(5):
    ax[i].imshow(get_input(img_path[i]))
    ax[i].axis('off')
    ax[i].set_title(img_path[i][-10:])
plt.show()

attr.iloc[:5]

plt.figure(figsize = (5,5))
sns.heatmap(attr.corr())

"""####Training VAE Dataset"""

import tensorflow as tf
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Input, Reshape, UpSampling2D, InputLayer, Lambda, ZeroPadding2D, Cropping2D, Conv2DTranspose, BatchNormalization
from keras.utils import to_categorical
from keras.losses import binary_crossentropy
from keras import backend as K
import random
from keras.losses import mse, binary_crossentropy
import os
import cv2
import zipfile
from PIL import Image
from matplotlib import image as mpimg
import shutil

img_sample=get_input(img_path[0])

with tf.compat.v1.Session() as sess:
    b_size = 128
    n_size = 512
    def sampling(args):
        z_mean, z_log_sigma = args
        epsilon = K.random_normal(shape = (n_size,) , mean = 0, stddev = 1)
        return z_mean + K.exp(z_log_sigma/2) * epsilon

    def build_conv_vae(input_shape, bottleneck_size, sampling, batch_size = 32):
      # ENCODER
        input = Input(shape=(input_shape[0],input_shape[1],input_shape[2]))
        x = Conv2D(32,(3,3),activation = 'relu', padding = 'same')(input)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2,2), padding ='same')(x)
        x = Conv2D(64,(3,3),activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2,2), padding ='same')(x)
        x = Conv2D(128,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2,2), padding ='same')(x)
        x = Conv2D(256,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D((2,2), padding ='same')(x)

        # Latent Variable Calculation
        shape = K.int_shape(x)
        flatten_1 = Flatten()(x)
        dense_1 = Dense(bottleneck_size, name='z_mean')(flatten_1)
        z_mean = BatchNormalization()(dense_1)
        flatten_2 = Flatten()(x)
        dense_2 = Dense(bottleneck_size, name ='z_log_sigma')(flatten_2)
        z_log_sigma = BatchNormalization()(dense_2)
        z = Lambda(sampling)([z_mean, z_log_sigma])
        encoder = Model(input, [z_mean, z_log_sigma, z], name = 'encoder')

        # DECODER
        latent_input = Input(shape=(bottleneck_size,), name = 'decoder_input')
        x = Dense(shape[1]*shape[2]*shape[3])(latent_input)
        x = Reshape((shape[1],shape[2],shape[3]))(x)
        x = UpSampling2D((2,2))(x)
        x = Cropping2D([[0,0],[0,1]])(x)
        x = Conv2DTranspose(256,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = UpSampling2D((2,2))(x)
        x = Cropping2D([[0,1],[0,1]])(x)
        x = Conv2DTranspose(128,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = UpSampling2D((2,2))(x)
        x = Cropping2D([[0,1],[0,1]])(x)
        x = Conv2DTranspose(64,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        x = UpSampling2D((2,2))(x)
        x = Conv2DTranspose(32,(3,3), activation = 'relu', padding = 'same')(x)
        x = BatchNormalization()(x)
        output = Conv2DTranspose(3,(3,3), activation = 'tanh', padding ='same')(x)
        decoder = Model(latent_input, output, name = 'decoder')

        output_2 = decoder(encoder(input)[2])
        vae = Model(input, output_2, name ='vae')
        return vae, encoder, decoder, z_mean, z_log_sigma

    vae_2, encoder, decoder, z_mean, z_log_sigma = build_conv_vae(img_sample.shape, n_size, sampling, batch_size = b_size)
    print("encoder summary:")
    encoder.summary()
    print("decoder summary:")
    decoder.summary()
    print("vae summary:")
    vae_2.summary()

    def vae_loss(input_img, output):
        # Compute error in reconstruction
        reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))

        # Compute the KL Divergence regularization term
        kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)

        # Return the average loss over all images in batch
        total_loss = (reconstruction_loss + 0.0001 * kl_loss)
        return total_loss
    vae_2.compile(optimizer='rmsprop', loss= vae_loss)
    encoder.compile(optimizer = 'rmsprop', loss = vae_loss)
    decoder.compile(optimizer = 'rmsprop', loss = vae_loss)
    history = vae_2.fit_generator(train_generator, steps_per_epoch = 4000, validation_data = val_generator, epochs=7, validation_steps= 500)

    x_test = []
    for i in range(64):
        x_test.append(get_input(img_path[random.randint(0,len(img_id))]))
    x_test = np.array(x_test)
    figure_Decoded = vae_2.predict(x_test.astype('float32')/127.5 -1, batch_size = b_size)
    figure_decoded = (figure_Decoded[0]+1)/2
    # Display decoder output images with size 128x128
    with zipfile.ZipFile('decoder_output.zip', 'w') as zipf:
      for i in range(5):
        plt.axis('off')
        plt.subplot(2, 5, 2 + i * 2)
        plt.imshow(cv2.resize((figure_Decoded[i] + 1) / 2, (128, 128)))
        plt.axis('off')

        # Simpan gambar ke dalam direktori tujuan
        image_path = f"decoder_output_{i}.png"
        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)
        plt.close()

        # Menghapus latar belakang putih dari gambar dan menyimpannya
        img = mpimg.imread(image_path)
        img = Image.fromarray((img * 255).astype(np.uint8))
        img = img.convert("RGBA")
        data = np.array(img)
        r, g, b, a = data[:, :, 0], data[:, :, 1], data[:, :, 2], data[:, :, 3]
        mask = (r >= 240) & (g >= 240) & (b >= 240) & (a >= 240)
        data[:, :, :3][mask] = [0, 0, 0]
        img = Image.fromarray(data)
        img.save(image_path)

        # Menambahkan gambar ke dalam file zip
        zipf.write(image_path)

        # Menghapus gambar setelah disimpan dalam file zip
        os.remove(image_path)
    plt.show()

    # Randomly generated 15 images from 15 series of noise information
    n = 3
    m = 5
    digit_size1 = 218
    digit_size2 = 178
    figure = np.zeros((digit_size1 * n, digit_size2 * m,3))

    for i in range(3):
        for j in range(5):
            z_sample = np.random.rand(1,512)
            x_decoded = decoder.predict([z_sample])
            figure[i * digit_size1: (i + 1) * digit_size1,
                   j * digit_size2: (j + 1) * digit_size2,:] = (x_decoded[0]+1)/2
    plt.figure(figsize=(10, 10))
    plt.imshow(figure)
    plt.show()
    plt.close()

    #kurva loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

    #Menyimpan model vae
    vae_2.save('model_vae.h5')

"""###Analisis Real Fake Images

####Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
#import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
np.random.seed(2)
from sklearn.model_selection import train_test_split
from skimage.io import imread, imshow
from sklearn.metrics import confusion_matrix
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping
from PIL import Image, ImageChops, ImageEnhance
import os
import pandas as pd
import itertools
import tensorflow as tf
from tensorflow import keras
import numpy as np
import cv2
from torchvision import datasets, transforms, models
from tensorflow.keras import layers

#Make sure Tensorflow is version 2.0 or higher
print('Tensorflow Version:', tf.__version__)

"""####Input dataset form GDrive"""

#Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Load Dataset
dataimg = '/content/drive/MyDrive/TA Regina/Dataset/CelebA'
files = os.listdir(dataimg)
images = []
for file in files:
  image = Image.open(os.path.join(dataimg, file))
  image = np.array(image.convert('L'))
  image = cv2.resize(image, (85, 85))
  images.append(image)
images = np.array(images) / 255.0

!unzip decoder_output.zip

!unzip Foto01\ \(2\).zip

!unzip Foto02.zip

def convert_to_ela_image(path, quality):
    temp_filename = 'temp_file_name.jpg'
    ela_filename = 'temp_ela.png'

    image = Image.open(path).convert('RGB')
    image.save(temp_filename, 'JPEG', quality = quality)
    temp_image = Image.open(temp_filename)

    ela_image = ImageChops.difference(image, temp_image)

    extrema = ela_image.getextrema()
    max_diff = max([ex[1] for ex in extrema])
    if max_diff == 0:
        max_diff = 1
    scale = 255.0 / max_diff

    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)

    return ela_image

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000871.jpg'
Image.open(image_real)

convert_to_ela_image(image_real, 90)

image_fake = '/content/Foto02/000871.jpg'
Image.open(image_fake)

convert_to_ela_image(image_fake, 90)

"""####Training & Validation Process"""

image_size = (85, 85)
def prepare_image(image_path):
    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0

X = [] # ELA converted images
Y = [] # 0 for fake, 1 for real

import random
path = '/content/drive/MyDrive/TA Regina/Dataset/CelebA'
for dirname, _, filenames in os.walk(path):
    for filename in filenames:
        if filename.endswith('jpg') or filename.endswith('png'):
            full_path = os.path.join(dirname, filename)
            X.append(prepare_image(full_path))
            Y.append(1)
            if len(Y) % 500 == 0:
                print(f'Processing {len(Y)} images')

random.shuffle(X)
X = X[:2100]
Y = Y[:2100]
print(len(X), len(Y))

path = '/content/Foto02'
for dirname, _, filenames in os.walk(path):
    for filename in filenames:
        if filename.endswith('jpg') or filename.endswith('png'):
            full_path = os.path.join(dirname, filename)
            X.append(prepare_image(full_path))
            Y.append(0)
            if len(Y) % 500 == 0:
                print(f'Processing {len(Y)} images')

print(len(X), len(Y))

X = np.array(X)
Y = to_categorical(Y, 2)
X = X.reshape(-1, 85, 85, 3)

X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)
X = X.reshape(-1,1,1,1)
print(len(X_train), len(Y_train))
print(len(X_val), len(Y_val))

"""#####Build Model Using Cov2D"""

def build_model():
    model = Sequential()
    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (85, 85, 3)))
    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (85, 85, 3)))
    model.add(MaxPool2D(pool_size = (2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(256, activation = 'relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation = 'softmax'))
    return model
model = build_model()
model.summary()

"""#####Training Process"""

epochs = 30
batch_size = 50

init_lr = 1e-4
optimizer = Adam(lr = init_lr, decay = init_lr/epochs)
model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])
early_stopping = EarlyStopping(monitor = 'val_acc',
                              min_delta = 0,
                              patience = 2,
                              verbose = 0,
                              mode = 'auto')
hist = model.fit(X_train,
                 Y_train,
                 batch_size = batch_size,
                 epochs = epochs,
                validation_data = (X_val, Y_val),
                callbacks = [early_stopping])

model.save('model_data.h5')

"""#####Curve Accuracy & Loss"""

# Plot the loss and accuracy curves for training and validation
fig, ax = plt.subplots(2,1)
ax[0].plot(hist.history['loss'], color='b', label="Training loss")
ax[0].plot(hist.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(hist.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(hist.history['val_accuracy'], color='r',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

"""#####Confusion Matrix"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Predict the values from the validation dataset
Y_pred = model.predict(X_val)
# Convert predictions classes to one hot vectors
Y_pred_classes = np.argmax(Y_pred,axis = 1)
# Convert validation observations to one hot vectors
Y_true = np.argmax(Y_val,axis = 1)
# compute the confusion matrix
confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)
# plot the confusion matrix
plot_confusion_matrix(confusion_mtx, classes = range(2))

"""####Images Prediction Real vs Fake"""

class_names = ['fake', 'real']

"""#####5 Prediksi Awal

######Prediksi 1
"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/024069.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/1.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 2"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000019.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/2.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 3"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000023.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/3.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 4"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000502.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/6.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 5"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000010.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/4.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""#####5 Prediksi Tahap 2

######Prediksi 6
"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000566.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/7.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 7"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000007.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/5.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 8"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000709.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/8.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 9"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000763.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/9.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 10"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000922.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto01/10.png'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""#####5 Prediksi Tahap 3

######Prediksi 11
"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000987.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000987.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 12"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/024098.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/024098.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 13"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000993.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000993.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 14"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000556.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000556.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 15"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000585.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000585.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""#####5 Prediksi Akhir

######Prediksi 16
"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000591.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000591.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 17"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000494.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000494.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 18"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000524.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000524.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 19"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000536.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000536.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""######Prediksi 20"""

image_real = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/000871.jpg'
image = prepare_image(image_real)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

image_fake = '/content/Foto02/000871.jpg'
image = prepare_image(image_fake)
image = image.reshape(-1, 85, 85, 3)
y_pred = model.predict(image)
y_pred_class = np.argmax(y_pred, axis = 1)[0]
print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')

"""###Get Metadata Image"""

from keras.preprocessing import image
from keras.applications import VGG16
from keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
from PIL import Image

# Load pre-trained VGG16 model (or any other DNN model)
model = VGG16(weights='imagenet', include_top=True)

# Path to your image
image_path = '/content/drive/MyDrive/TA Regina/Dataset/CelebA/024069.jpg'

# Load and preprocess the image
img = Image.open(image_path).resize((224, 224))
x = np.array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# Extract metadata using the DNN model
predictions = model.predict(x)
predicted_classes = decode_predictions(predictions, top=3)[0]

# Print the top predicted classes
for _, class_name, probability in predicted_classes:
    print(f"Class: {class_name}, Probability: {probability}")

from keras.preprocessing import image
from keras.models import load_model
import numpy as np
from PIL import Image
import tensorflow.keras.backend as K
import tensorflow as tf

def vae_loss(input_img, output):
  # Compute error in reconstruction
  reconstruction_loss = mse(K.flatten(input_img) , K.flatten(output))

  # Compute the KL Divergence regularization term
  kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = -1)

  # Return the average loss over all images in batch
  total_loss = (reconstruction_loss + 0.0001 * kl_loss)
  return total_loss

# Memasukkan fungsi kehilangan ke dalam lingkup saat memuat model
with K.name_scope('custom_loss'):
    setattr(K, 'vae_loss', vae_loss)
    with tf.keras.utils.custom_object_scope({'vae_loss': vae_loss}):
        model = load_model('model_vae.h5')

# Path to your generated image
image_path = '/content/decoder_output/decoder_output_32.png'

# Load and preprocess the generated image
img = Image.open(image_path).resize((178, 218))
img = img.convert("RGB")
x = np.array(img)
x = np.expand_dims(x, axis=0)
x = x / 255.0  # Normalisasi piksel menjadi rentang [0, 1]

# Extract metadata using the VAE model
latent_vector = model.predict(x)

# Print the latent vector
print("Latent vector:", latent_vector)

!pip install exifread

import exifread
from PIL import Image

# Fungsi untuk membaca metadata EXIF
def read_exif(image_path):
    with open(image_path, 'rb') as f:
        tags = exifread.process_file(f)
    return tags

# Fungsi untuk mendapatkan informasi gambar
def get_image_info(image_path):
    # Membaca metadata EXIF
    exif_tags = read_exif(image_path)

    # Membuka gambar menggunakan PIL
    image = Image.open(image_path)

    # Mendapatkan informasi dasar gambar
    width, height = image.size
    format = image.format
    mode = image.mode

    # Membaca informasi tambahan dari metadata EXIF
    make = exif_tags.get('Image Make')
    model = exif_tags.get('Image Model')
    exposure_time = exif_tags.get('EXIF ExposureTime')
    aperture = exif_tags.get('EXIF FNumber')
    iso = exif_tags.get('EXIF ISOSpeedRatings')

    # Mencetak informasi gambar
    print("Image Path:", image_path)
    print("Width:", width)
    print("Height:", height)
    print("Format:", format)
    print("Mode:", mode)
    print("Make:", make)
    print("Model:", model)
    print("Exposure Time:", exposure_time)
    print("Aperture:", aperture)
    print("ISO:", iso)

# Path ke gambar yang ingin dianalisis
image_path = "/content/decoder_output/decoder_output_32.png"

# Memanggil fungsi untuk mendapatkan informasi gambar
get_image_info(image_path)

import exifread
from PIL import Image

# Fungsi untuk membaca metadata EXIF
def read_exif(image_path):
    with open(image_path, 'rb') as f:
        tags = exifread.process_file(f)
    return tags

# Fungsi untuk mendapatkan informasi gambar
def get_image_info(image_path):
    # Membaca metadata EXIF
    exif_tags = read_exif(image_path)

    # Membuka gambar menggunakan PIL
    image = Image.open(image_path)

    # Mendapatkan informasi dasar gambar
    width, height = image.size
    format = image.format
    mode = image.mode

    # Membaca informasi tambahan dari metadata EXIF
    make = exif_tags.get('Image Make')
    model = exif_tags.get('Image Model')
    exposure_time = exif_tags.get('EXIF ExposureTime')
    aperture = exif_tags.get('EXIF FNumber')
    iso = exif_tags.get('EXIF ISOSpeedRatings')

    # Mencetak informasi gambar
    print("Image Path:", image_path)
    print("Width:", width)
    print("Height:", height)
    print("Format:", format)
    print("Mode:", mode)
    print("Make:", make)
    print("Model:", model)
    print("Exposure Time:", exposure_time)
    print("Aperture:", aperture)
    print("ISO:", iso)

# Path ke gambar yang ingin dianalisis
image_path = "/content/drive/MyDrive/TA Regina/Dataset/CelebA/024069.jpg"

# Memanggil fungsi untuk mendapatkan informasi gambar
get_image_info(image_path)